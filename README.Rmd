---
title: "Bellabeat Case Study"
author: Aram Soylemez
output: md_document
---

### 1.Summary

Bellabeat is high-tech company which produces smart products for health tracking use for women. They have different types of devices such as smart watches, smart bottles and so on which through the company aims to easen the burden of the daily life by tracking real life data.

For this analysis we are going to focus on one of the companies products which is Bellabeat App. This app gathers data from Bellabeat products and have a great potential to improve the users experience of everyday life.

### 2. Asking Phase 

We are using third part data to identify the trends in similar use of smart devices.

### 3. Preparing Phase

#### 3.1. About Data 

We are using FitBit Fitness Tracker Data. The data is stored on Kaggle and provided by Mobius. It is provided by the owner of the data to the public use without any demands. The dataset is created by Amazon Mechanical Turk between 03.12.2016-05.12.2016. Thirty user has consented to share their data with public.

#### 3.2. Limitations

The dataset that we are using consists of 18 CSV file which covers 30 users daily and hourly activities over two months. As the dataset only covers 30 users data it might have sampling bias, some tables such as the one on weight data only contains 8 users data. Therefore we can conclude that some of the tables are not very helpful in a strong analysis process. Laslty, the data was collected only in between two months which is also can be considered as quite limited.

### 4. Processing Phase

For all the steps in this procejt I used R, both for data analysis and for data visualisation.

#### 4.1 Pacgages Used

For this project beside the base pachage I used these packages:

* tidyverse
* lubridate
* janitor

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(lubridate)
library(janitor)
```
#### 4.2. Importing Tables

Since weight and heartrate tables do not cover the majority of the users I have chosen to work over only the tables covering calorie, step, activity and sleep data.

```{r}
daily_calorie <- read_csv("~/Desktop/google_case_study/Fitabase Data 4.12.16-5.12.16/dailyCalories_merged.csv")
daily_activity <- read_csv("~/Desktop/google_case_study/Fitabase Data 4.12.16-5.12.16/dailyActivity_merged.csv")
daily_step <- read_csv("~/Desktop/google_case_study/Fitabase Data 4.12.16-5.12.16/dailySteps_merged.csv")
daily_sleep <- read_csv("~/Desktop/google_case_study/Fitabase Data 4.12.16-5.12.16/sleepDay_merged.csv")
```

#### 4.3. Previewing The Data Sets

First we should check the data set and skimm the simple summary of each table.

```{r}
head(daily_activity)
str(daily_activity)

head(daily_calorie)
str(daily_calorie)

head(daily_sleep)
str(daily_sleep)

head(daily_step)
str(daily_step) 
```
#### 4.4. Data Wrangling

For this step I inspected the data thoroughly and checked for duplicates, data types, missing datas, date data, number of users. Then I re-adjusted the column names.

##### 4.4.1. Number of Users

Before proceeding for the rest of the analysis we should check the number of users we have in our data because in the end we are going to merge the entire four table for the rest of the analysis.

```{r}
n_distinct(daily_activity$Id)
n_distinct(daily_sleep$Id)
n_distinct(daily_step$Id)
n_distinct(daily_calorie$Id)
```
As it is seen except for the sleep data we have 33 distinct users.

##### 4.4.2. Duplicate and N/A Data Points

In order to have valid results to a certain degree we must delete the duplicate points. Missing data can be handeled within big datasets, however, since we have small dataset we should avoid using it as well.

```{r}
sum(duplicated(daily_activity))
sum(duplicated(daily_calorie))
sum(duplicated(daily_step))
sum(duplicated(daily_sleep))
```
It looks like we only have 3 duplicates in sleep table. As we are going to remove all the duplicates and missing data we can write a simple function to do all these for us. I will add this function into a one final function, this is just for showing purposes.

```{r}
data_fiixer <- function(data) {
  # remove the missing data
  data <- na.omit(data)
  
  # remove the duplicates
  data <- distinct(data)
  
  return(data)
}
```
##### 4.4.3. Cleaning Names and Rename Columns

For the sake of the analysis we should standardize the names and the columns so the for the analysis phase we can increase the comfort.

As we are going to apply the same process for all the tables we can write another function to ease and fasten our job. For this we can write a total function which can handle both the renaming and time adjusting tasks.

##### 4.4.4. Consistency of Time Frame

We are going to merge all the data based on both Id and Time data therefore we require our data to be consistent in both cases.

We can again write a function to secure the consistency of our data. To fasten the process I have joined all the functions together. Which is better in terms of time control and ram and cpu usage as well.
```{r}

  #As Sleep table includes hours and seconds which we do not need for this analysis we should fix it first.
daily_sleep$SleepDay <- substr(daily_sleep$SleepDay, 1, regexpr(" ", daily_sleep$SleepDay) - 1)

data_fixer <- function(data) {
  
  # Remove duplicates and missing data
  data <- data %>%
    distinct() %>%
    drop_na()
  
  # Cleaning Names
  clean_names(data)
  
  # Renaming Columns
  data <- rename_with(data, tolower)
  
  # All the time data is stored in the second column. The name of the second column
  col_name <- colnames(data)[2]
  
  # Rename and Mutate the Time Data
  data <- data %>% 
    rename(date = col_name) %>% 
    mutate(date = as_date(date, format = "%m/%d/%Y"))
  
  return(data)
}

daily_activity <- data_fixer(daily_activity)
daily_calorie <- data_fixer(daily_calorie)
daily_sleep <- data_fixer(daily_sleep)
daily_step <- data_fixer(daily_step)
```

Before we join the datasets lets have look at them to soo if there is any problem.
```{r}
glimpse(c(daily_activity, daily_calorie, daily_sleep, daily_step))
```
It looks fine, so we can proceed for the next phase. However, it looks like we already have the calorie information in activity table as we do not need the daily_calories table we can leave it out.

#### 4.4.5. Joining Datasets

We are going to merge daily_activity, daily_calories and daily_sleep data by the id and the date variables. The aim of this move is to be able to look for relationship between the variables.

```{r}
fitbit_merged <- daily_activity %>% 
  left_join(daily_sleep, by=c("id", "date")) %>% 
  drop_na()

glimpse(fitbit_merged)
```
The reason why we used drop_na() function again is because after using left_join() function R adds the second table to the first one, as the number of users does not match in both table there happens to be some missing data points. Due to my needs for activity data to be on the beginning of the table I decided to write the code in this way, otherwise we could have just changed the order of the tables, meaning we could have written the code in this way:
```{r}
# fitbit_merged <- daily_sleep %>% 
#  left_join(daily_activity, by=c("id", "date"))
```

### 5. Analyzing and Sharing Phase

We will conduct the analysis of the tables to determine the trends and decide if the results can lead to a profitable decisions.

#### 5.1. Seperating the Users into Groups by Their Activity Level

Since the demographic data which could have been useful during the analysis is not reachable we need a standing point for the rest of the analysis. For that we can use the number of steps taken every day.


    *Sedentary - Less than 5000 steps a day.
    *Lightly active - Between 5000 and 7499 steps a day.
    *Fairly active - Between 7500 and 9999 steps a day.
    *Very active - More than 10000 steps a day.

This classification was made based on the article on this [link](https://www.10000steps.org.au/articles/counting-steps/). 

Following this we need the daily avarage of each users:
```{r}
avarage_steps <- fitbit_merged %>% 
  group_by(id) %>% 
  summarize(avg_steps = mean(totalsteps), avg_calories = mean(calories), avg_sleep_hr = mean(totalminutesasleep/60))

head(avarage_steps)
```
Based on this table now we can classify our users:

```{r}
user_class <- avarage_steps %>% 
  mutate(type = case_when(
    avg_steps < 5000 ~ "sedantary",
    avg_steps >= 5000 & avg_steps <= 7499 ~ "lightly active",
    avg_steps >- 7500 & avg_steps <= 9999 ~ "fairly active",
    avg_steps >= 10000 ~ "very active"
  ))

head(user_class)
```
We can easily show the percentage of each user type with a simple data visualisation in a pie chart. First lets calculate the percentage of each user type and save it within a table.

```{r}
user_class_p <- user_class %>% 
  group_by(type) %>% 
  summarise(total = n()) %>% 
  mutate(percentage = ceiling((total / sum(total))*100))

head(user_class_p)
```

As you can see the users are distributed by their activities. the total percentage makes up to it is because we ceiled the percentages to make working with them easier. The data shows that there is a relationship between the usage of the app and the type of users. *Devices are more popular among fairly active users.*

We can see this by creating a pie chart:
```{r}
user_class_p %>% 
  ggplot(mapping = aes(x="", y = percentage, fill = type)) +
  geom_bar(stat = "identity", width = 1) +
  coord_polar(theta = "y") + # This converts the bar chart to a pie chart
  labs(title = "Distribution of User Type", fill = "Types") +
  theme_minimal() +
  theme(axis.title.x= element_blank(),
        axis.title.y = element_blank(),
        panel.border = element_blank(), 
        panel.grid = element_blank(), 
        axis.ticks = element_blank(),
        axis.text.x = element_blank(),
        plot.title = element_text(hjust = 0.5, size=14, face = "bold")) +
  geom_text(aes(label = paste0(percentage,"%")),
            position = position_stack(vjust = 0.5))
ggsave(filename = paste0("~/Desktop/google_case_study/visuals/", "dist_of_users.png"))
```

#### 5.2. Sleeping and Stepping Data

Before we start to look for some results aiming for a relation we should inspect the sleeping and stepping distribution throughout the week.
```{r}
weekly_step_sleep <- fitbit_merged %>%
  mutate(days = weekdays(date))

weekly_step_sleep$days <- ordered(weekly_step_sleep$days, levels=c("Monday", "Tuesday", "Wednesday", "Thursday","Friday", "Saturday", "Sunday"))

weekly_step_sleep <- weekly_step_sleep %>% 
  group_by(days) %>% 
  summarise(avr_steps = mean(totalsteps), avr_sleep = mean(totalminutesasleep/60))

head(weekly_step_sleep)
```
Based on this data now we can see the average steps and sleeping hours on a bar chart.
```{r}
ggplot(data=weekly_step_sleep, mapping = aes(x=days, y=avr_steps)) +
  geom_bar(stat="identity")

ggplot(data=weekly_step_sleep, mapping = aes(x=days, y=avr_sleep)) +
  geom_bar(stat="identity")

```
The above tabels shows that while the users are walking the recommended amount everyday except for sunday they do not sleep the recommended amount of hours which is eight hours.

#### 5.3. Avarage Sleeping Hours and Activity Level

Sleeping is an important part of our lives and and it has a huge impact on both activity and energy. It is needed to be checked.

```{r}
sleep_type <- user_class %>% 
  group_by(type) %>% 
  summarise(average = mean(avg_sleep_hr))

head(sleep_type)
  
```
Here we can summarize it with a bar chart.
```{r}
sleep_type %>%
  ggplot(mapping = aes(x = type, y = average, fill = type)) +
  geom_bar(stat = "identity") +
  labs(title = "Average Sleeping and Activity Level")
ggsave(filename = paste0("~/Desktop/google_case_study/visuals/", "sleep_and_type.png"))
```
As it can be seen through the table as well, while lightly active users are sleeping less and sedantary active users are sleeping the most, very and fairly active users are sleeping in between desirable hour values.

#### 5.4. Correlations

We are going to check the correlations between:

* Steps and calories burned
* Steps and sleeping hours
* Activity level and app usage

For this, first we should calculate the app usage values first and add it to the user_class table.
```{r}
user_class <- user_class %>% 
  left_join(fitbit_merged %>% 
              group_by(id) %>% 
              summarise(days_used=sum(n())) %>% 
              mutate(usage = case_when(
                days_used >= 1 & days_used <= 10 ~ "low use",
                days_used >= 11 & days_used <= 20 ~ "moderate use", 
                days_used >= 21 & days_used <= 31 ~ "high use", 
              )))

head(user_class)
```
Now we have activity and app usage columns with average sleeping, walking and burned calories data all together on one table, we can conclude through the analysis of this table.
```{r}
cor_steps_cal <- cor(user_class$avg_steps, user_class$avg_calories)
cor_steps_slp <- cor(user_class$avg_steps, user_class$avg_sleep_hr)
cor_type_use <- cor(user_class$avg_steps, user_class$days_used)

cor_steps_cal
cor_steps_slp
cor_type_use
```
As the correlation analysis shows there is a moderately week positive relationship (0.35) between steps taken and calories burned. This might be a mistake, therefore this must be considered as an important outcome where the developers must take a second look.
```{r}
user_class %>% 
  ggplot(mapping = aes(x=avg_steps, y=avg_calories)) +
  geom_point() +
  geom_smooth(method = "lm")
```

For the steps taken and sleeping hours there is a strong negative relationship (-0.66) where the number of steps are increasing there is a decrease in sleeping hours.
```{r}
user_class %>% 
  ggplot(mapping = aes(x=avg_steps, y=avg_sleep_hr)) +
  geom_point() +
  geom_smooth(method = "lm")
```

Last but not least there is a moderately week positive relationship (0,27) between the actity level and app usage, yet it is not a sufficent data for creating a data driven business decision.

```{r echo=TRUE, message=FALSE, warning=FALSE}
user_class %>% 
  ggplot(mapping = aes(x=avg_steps, y=days_used)) +
  geom_point() +
  geom_smooth(method= "lm")
```
Now the previous phases are done we can continue with the conclusion.

### 6. Conclusion

Bellabeat's aim is to empower women by increasing the daily life experience through their smart devices.

As for this case study it can be said that the dataset was not sufficient enough for an overall analysis. The dataset includes 33 users data, however, when we clear the dataset and finally merge them together the remaining dataset consists of only 24 users data. My first and most useful insight would be to come up with bigger data and also it's covering an entire year might be also quite usefull for answering business related questions.

The foremost result of my analysis is showing that the people who uses the app more frequently are showing better results in daily activity, calories burned and sleeping hours. Therefore the main point to be worked on must be over increasing the average app usage.

For this we might apply:
* Giving feedback for activities on a daily, weekly and monthly basis.
* Sending notifications over recommended and aimed activities.
* Creating a competing environment might motivate the users.
* Making the app more fun.


